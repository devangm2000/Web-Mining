{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1=request.urlopen('https://en.wikipedia.org/wiki/Web_mining').read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup1 = BeautifulSoup(r1,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Web mining is the application of data mining techniques to discover patterns from the World Wide Web. As the name proposes, this is information gathered by mining the web. It makes utilization of automated apparatuses to reveal and extricate data from servers and web2 reports, and it permits organizations to get to both organized and unstructured information from browser activities, server logs, website and link structure, page content and different sources.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulttext1 = soup1.find('p')\n",
    "resulttext1.get_text()\n",
    "# soup1 = BeautifulSoup(resulttext,'html.parser').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data mining is a process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal to extract information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1]\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2=request.urlopen('https://en.wikipedia.org/wiki/Data_mining').read().decode('utf8')\n",
    "from bs4 import BeautifulSoup\n",
    "soup2 = BeautifulSoup(r2,'html.parser')\n",
    "resulttext2 = soup2.find('p')\n",
    "resulttext2.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import pandas as pd\n",
    "import re\n",
    "stop_words = (stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words.append('.') \n",
    "stop_words.append(',') \n",
    "stop_words.append('/') \n",
    "stop_words.append('!') \n",
    "stop_words.append('@') \n",
    "stop_words.append('#') \n",
    "stop_words.append('$') \n",
    "stop_words.append('%') \n",
    "stop_words.append('^') \n",
    "stop_words.append('&') \n",
    "stop_words.append('*') \n",
    "stop_words.append('(')\n",
    "stop_words.append(')') \n",
    "stop_words.append('_')\n",
    "stop_words.append('-')\n",
    "stop_words.append('+')\n",
    "stop_words.append('=')\n",
    "stop_words.append('[')\n",
    "stop_words.append(']')\n",
    "stop_words.append(';')\n",
    "stop_words.append('{')\n",
    "stop_words.append('}')\n",
    "stop_words.append('~')\n",
    "stop_words.append('`')\n",
    "stop_words.append('``')\n",
    "stop_words.append(\"''\")\n",
    "word_tokens1 = word_tokenize(resulttext1.get_text()) \n",
    "word_tokens2 = word_tokenize(resulttext2.get_text()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Web',\n",
       "  'mining',\n",
       "  'is',\n",
       "  'the',\n",
       "  'application',\n",
       "  'of',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'techniques',\n",
       "  'to',\n",
       "  'discover',\n",
       "  'patterns',\n",
       "  'from',\n",
       "  'the',\n",
       "  'World',\n",
       "  'Wide',\n",
       "  'Web',\n",
       "  '.',\n",
       "  'As',\n",
       "  'the',\n",
       "  'name',\n",
       "  'proposes',\n",
       "  ',',\n",
       "  'this',\n",
       "  'is',\n",
       "  'information',\n",
       "  'gathered',\n",
       "  'by',\n",
       "  'mining',\n",
       "  'the',\n",
       "  'web',\n",
       "  '.',\n",
       "  'It',\n",
       "  'makes',\n",
       "  'utilization',\n",
       "  'of',\n",
       "  'automated',\n",
       "  'apparatuses',\n",
       "  'to',\n",
       "  'reveal',\n",
       "  'and',\n",
       "  'extricate',\n",
       "  'data',\n",
       "  'from',\n",
       "  'servers',\n",
       "  'and',\n",
       "  'web2',\n",
       "  'reports',\n",
       "  ',',\n",
       "  'and',\n",
       "  'it',\n",
       "  'permits',\n",
       "  'organizations',\n",
       "  'to',\n",
       "  'get',\n",
       "  'to',\n",
       "  'both',\n",
       "  'organized',\n",
       "  'and',\n",
       "  'unstructured',\n",
       "  'information',\n",
       "  'from',\n",
       "  'browser',\n",
       "  'activities',\n",
       "  ',',\n",
       "  'server',\n",
       "  'logs',\n",
       "  ',',\n",
       "  'website',\n",
       "  'and',\n",
       "  'link',\n",
       "  'structure',\n",
       "  ',',\n",
       "  'page',\n",
       "  'content',\n",
       "  'and',\n",
       "  'different',\n",
       "  'sources',\n",
       "  '.'],\n",
       " ['Data',\n",
       "  'mining',\n",
       "  'is',\n",
       "  'a',\n",
       "  'process',\n",
       "  'of',\n",
       "  'discovering',\n",
       "  'patterns',\n",
       "  'in',\n",
       "  'large',\n",
       "  'data',\n",
       "  'sets',\n",
       "  'involving',\n",
       "  'methods',\n",
       "  'at',\n",
       "  'the',\n",
       "  'intersection',\n",
       "  'of',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  ',',\n",
       "  'statistics',\n",
       "  ',',\n",
       "  'and',\n",
       "  'database',\n",
       "  'systems',\n",
       "  '.',\n",
       "  '[',\n",
       "  '1',\n",
       "  ']',\n",
       "  'Data',\n",
       "  'mining',\n",
       "  'is',\n",
       "  'an',\n",
       "  'interdisciplinary',\n",
       "  'subfield',\n",
       "  'of',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'and',\n",
       "  'statistics',\n",
       "  'with',\n",
       "  'an',\n",
       "  'overall',\n",
       "  'goal',\n",
       "  'to',\n",
       "  'extract',\n",
       "  'information',\n",
       "  '(',\n",
       "  'with',\n",
       "  'intelligent',\n",
       "  'methods',\n",
       "  ')',\n",
       "  'from',\n",
       "  'a',\n",
       "  'data',\n",
       "  'set',\n",
       "  'and',\n",
       "  'transform',\n",
       "  'the',\n",
       "  'information',\n",
       "  'into',\n",
       "  'a',\n",
       "  'comprehensible',\n",
       "  'structure',\n",
       "  'for',\n",
       "  'further',\n",
       "  'use',\n",
       "  '.',\n",
       "  '[',\n",
       "  '1',\n",
       "  ']',\n",
       "  '[',\n",
       "  '2',\n",
       "  ']',\n",
       "  '[',\n",
       "  '3',\n",
       "  ']',\n",
       "  '[',\n",
       "  '4',\n",
       "  ']',\n",
       "  'Data',\n",
       "  'mining',\n",
       "  'is',\n",
       "  'the',\n",
       "  'analysis',\n",
       "  'step',\n",
       "  'of',\n",
       "  'the',\n",
       "  '``',\n",
       "  'knowledge',\n",
       "  'discovery',\n",
       "  'in',\n",
       "  'databases',\n",
       "  \"''\",\n",
       "  'process',\n",
       "  ',',\n",
       "  'or',\n",
       "  'KDD',\n",
       "  '.',\n",
       "  '[',\n",
       "  '5',\n",
       "  ']',\n",
       "  'Aside',\n",
       "  'from',\n",
       "  'the',\n",
       "  'raw',\n",
       "  'analysis',\n",
       "  'step',\n",
       "  ',',\n",
       "  'it',\n",
       "  'also',\n",
       "  'involves',\n",
       "  'database',\n",
       "  'and',\n",
       "  'data',\n",
       "  'management',\n",
       "  'aspects',\n",
       "  ',',\n",
       "  'data',\n",
       "  'pre-processing',\n",
       "  ',',\n",
       "  'model',\n",
       "  'and',\n",
       "  'inference',\n",
       "  'considerations',\n",
       "  ',',\n",
       "  'interestingness',\n",
       "  'metrics',\n",
       "  ',',\n",
       "  'complexity',\n",
       "  'considerations',\n",
       "  ',',\n",
       "  'post-processing',\n",
       "  'of',\n",
       "  'discovered',\n",
       "  'structures',\n",
       "  ',',\n",
       "  'visualization',\n",
       "  ',',\n",
       "  'and',\n",
       "  'online',\n",
       "  'updating',\n",
       "  '.',\n",
       "  '[',\n",
       "  '1',\n",
       "  ']'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens1,word_tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = [w for w in word_tokens1 if not w in stop_words] \n",
    "words2 = [w for w in word_tokens2 if not w in stop_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Web',\n",
       "  'mining',\n",
       "  'application',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'techniques',\n",
       "  'discover',\n",
       "  'patterns',\n",
       "  'World',\n",
       "  'Wide',\n",
       "  'Web',\n",
       "  'As',\n",
       "  'name',\n",
       "  'proposes',\n",
       "  'information',\n",
       "  'gathered',\n",
       "  'mining',\n",
       "  'web',\n",
       "  'It',\n",
       "  'makes',\n",
       "  'utilization',\n",
       "  'automated',\n",
       "  'apparatuses',\n",
       "  'reveal',\n",
       "  'extricate',\n",
       "  'data',\n",
       "  'servers',\n",
       "  'web2',\n",
       "  'reports',\n",
       "  'permits',\n",
       "  'organizations',\n",
       "  'get',\n",
       "  'organized',\n",
       "  'unstructured',\n",
       "  'information',\n",
       "  'browser',\n",
       "  'activities',\n",
       "  'server',\n",
       "  'logs',\n",
       "  'website',\n",
       "  'link',\n",
       "  'structure',\n",
       "  'page',\n",
       "  'content',\n",
       "  'different',\n",
       "  'sources'],\n",
       " ['Data',\n",
       "  'mining',\n",
       "  'process',\n",
       "  'discovering',\n",
       "  'patterns',\n",
       "  'large',\n",
       "  'data',\n",
       "  'sets',\n",
       "  'involving',\n",
       "  'methods',\n",
       "  'intersection',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'statistics',\n",
       "  'database',\n",
       "  'systems',\n",
       "  '1',\n",
       "  'Data',\n",
       "  'mining',\n",
       "  'interdisciplinary',\n",
       "  'subfield',\n",
       "  'computer',\n",
       "  'science',\n",
       "  'statistics',\n",
       "  'overall',\n",
       "  'goal',\n",
       "  'extract',\n",
       "  'information',\n",
       "  'intelligent',\n",
       "  'methods',\n",
       "  'data',\n",
       "  'set',\n",
       "  'transform',\n",
       "  'information',\n",
       "  'comprehensible',\n",
       "  'structure',\n",
       "  'use',\n",
       "  '1',\n",
       "  '2',\n",
       "  '3',\n",
       "  '4',\n",
       "  'Data',\n",
       "  'mining',\n",
       "  'analysis',\n",
       "  'step',\n",
       "  'knowledge',\n",
       "  'discovery',\n",
       "  'databases',\n",
       "  'process',\n",
       "  'KDD',\n",
       "  '5',\n",
       "  'Aside',\n",
       "  'raw',\n",
       "  'analysis',\n",
       "  'step',\n",
       "  'also',\n",
       "  'involves',\n",
       "  'database',\n",
       "  'data',\n",
       "  'management',\n",
       "  'aspects',\n",
       "  'data',\n",
       "  'pre-processing',\n",
       "  'model',\n",
       "  'inference',\n",
       "  'considerations',\n",
       "  'interestingness',\n",
       "  'metrics',\n",
       "  'complexity',\n",
       "  'considerations',\n",
       "  'post-processing',\n",
       "  'discovered',\n",
       "  'structures',\n",
       "  'visualization',\n",
       "  'online',\n",
       "  'updating',\n",
       "  '1'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words1,words2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc1.doc', 'w') as f:\n",
    "    for item in words1:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "with open('doc2.doc', 'w') as f:\n",
    "    for item in words2:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "readwords1 = []\n",
    "readwords2 = []\n",
    "# opening the text file \n",
    "with open('doc1.doc','r') as file: \n",
    "   \n",
    "    # reading each line     \n",
    "    for line in file: \n",
    "   \n",
    "        # reading each word         \n",
    "        for word in line.split(): \n",
    "   \n",
    "            # displaying the words            \n",
    "            readwords1.append(word)\n",
    "# opening the text file \n",
    "with open('doc2.doc','r') as file: \n",
    "   \n",
    "    # reading each line     \n",
    "    for line in file: \n",
    "   \n",
    "        # reading each word         \n",
    "        for word in line.split(): \n",
    "   \n",
    "            # displaying the words            \n",
    "            readwords2.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \" \".join(readwords1)\n",
    "text2 = \" \".join(readwords2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Web mining application data mining techniques discover patterns World Wide Web As name proposes information gathered mining web It makes utilization automated apparatuses reveal extricate data servers web2 reports permits organizations get organized unstructured information browser activities server logs website link structure page content different sources'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data mining process discovering patterns large data sets involving methods intersection machine learning statistics database systems 1 Data mining interdisciplinary subfield computer science statistics overall goal extract information intelligent methods data set transform information comprehensible structure use 1 2 3 4 Data mining analysis step knowledge discovery databases process KDD 5 Aside raw analysis step also involves database data management aspects data pre-processing model inference considerations interestingness metrics complexity considerations post-processing discovered structures visualization online updating 1'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Web mining application data mining techniques discover patterns World Wide Web As name proposes information gathered mining web It makes utilization automated apparatuses reveal extricate data servers web2 reports permits organizations get organized unstructured information browser activities server logs website link structure page content different sourcesData mining process discovering patterns large data sets involving methods intersection machine learning statistics database systems 1 Data mining interdisciplinary subfield computer science statistics overall goal extract information intelligent methods data set transform information comprehensible structure use 1 2 3 4 Data mining analysis step knowledge discovery databases process KDD 5 Aside raw analysis step also involves database data management aspects data pre-processing model inference considerations interestingness metrics complexity considerations post-processing discovered structures visualization online updating 1'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [text1,text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Words</th>\n",
       "      <th>Doc 1</th>\n",
       "      <th>Doc 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Web</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mining</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>application</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mining</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>structures</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>visualization</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>online</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>updating</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Words  Doc 1  Doc 2\n",
       "0              Web      1      0\n",
       "1           mining      1      1\n",
       "2      application      1      0\n",
       "3             data      1      1\n",
       "4           mining      1      1\n",
       "..             ...    ...    ...\n",
       "117     structures      0      1\n",
       "118  visualization      0      1\n",
       "119         online      0      1\n",
       "120       updating      0      1\n",
       "121              1      0      1\n",
       "\n",
       "[122 rows x 3 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "booldoc1 = []\n",
    "booldoc2 = []\n",
    "for x in (text1 + text2).split(\" \"):\n",
    "    if(x in text1.split(\" \")):\n",
    "        booldoc1.append(1)\n",
    "    else:\n",
    "        booldoc1.append(0)\n",
    "for x in (text1 + text2).split(\" \"):\n",
    "    if(x in text2.split(\" \")):\n",
    "        booldoc2.append(1)\n",
    "    else:\n",
    "        booldoc2.append(0)\n",
    "booldata = {\"Words\":(text1 + text2).split(\" \"),\"Doc 1\":booldoc1,\"Doc 2\":booldoc2}\n",
    "pd.DataFrame(booldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vec.fit_transform(docs)\n",
    "X = X.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df =pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "#pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df =pd.DataFrame(X.toarray(), columns=['doc1 Freq','doc2 Freq'])\n",
    "df['words'] = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>doc1 Freq</th>\n",
       "      <th>doc2 Freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>activities</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>also</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>analysis</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>apparatuses</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>application</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>web</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>web2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>website</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>wide</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>world</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          words  doc1 Freq  doc2 Freq\n",
       "0    activities          1          0\n",
       "1          also          0          1\n",
       "2      analysis          0          2\n",
       "3   apparatuses          1          0\n",
       "4   application          1          0\n",
       "..          ...        ...        ...\n",
       "85          web          3          0\n",
       "86         web2          1          0\n",
       "87      website          1          0\n",
       "88         wide          1          0\n",
       "89        world          1          0\n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(df)\n",
    "df[['words','doc1 Freq','doc2 Freq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Web</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Web</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mining</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mining</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>application</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         words  Position\n",
       "0          Web         0\n",
       "1          Web        75\n",
       "2       mining         4\n",
       "4       mining        28\n",
       "5  application        11"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compwords1 =[]\n",
    "pos1=[]\n",
    "for x in (text1+text2).split(\" \"):\n",
    "    m =0\n",
    "    if x in words1:\n",
    "        for y in text1.split(\" \"):\n",
    "            m += 1 \n",
    "            if(x==y):\n",
    "                compwords1.append(x)\n",
    "                pos1.append(text1.index(x,m-1)) \n",
    "finaldata1 = {\"words\":compwords1,\"Position\":pos1}\n",
    "h = pd.DataFrame(finaldata1)\n",
    "h.drop_duplicates(subset=['words', 'Position'],inplace=True)\n",
    "h.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mining</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mining</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>data</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>patterns</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       words  Position\n",
       "0     mining         5\n",
       "1     mining       140\n",
       "3       data        47\n",
       "5       data       116\n",
       "10  patterns        32"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compwords2 =[]\n",
    "pos2=[]\n",
    "for x in resulttext1.get_text().split(\" \"):\n",
    "    m =0\n",
    "    if x in words2:\n",
    "        for y in text2.split(\" \"):\n",
    "            m += 1 \n",
    "            if(x==y):\n",
    "                compwords2.append(x)\n",
    "                pos2.append(text2.index(x,m-1)) \n",
    "finaldata2 = {\"words\":compwords2,\"Position\":pos2}\n",
    "\n",
    "h = pd.DataFrame(finaldata2)\n",
    "h.drop_duplicates(subset=['words', 'Position'],inplace=True)\n",
    "h.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2 is more relevent\n"
     ]
    }
   ],
   "source": [
    "searchstring = \"Web hello intersection data is analysis interdisciplinary subfield usefull\"\n",
    "\n",
    "sc1=0\n",
    "sc2=0\n",
    "for x in searchstring.split(\" \"):\n",
    "        if(x in text1.split(\" \")):\n",
    "            sc1=sc1+1\n",
    "        if(x in text2.split(\" \")):\n",
    "            sc2=sc2+1\n",
    "\n",
    "if(sc1>sc2):\n",
    "    print(\"Doc1 is more relevent\")\n",
    "elif(sc1==sc2):\n",
    "    print(\"Both Documents are equally relevent\")\n",
    "elif(sc1<sc2):\n",
    "    print(\"Doc2 is more relevent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW IS NOT USEFUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "import numpy as np  \n",
    "import random  \n",
    "import string\n",
    "\n",
    "import bs4 as bs  \n",
    "import urllib.request  \n",
    "import re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq1 = {}\n",
    "wordfreq2 = {}\n",
    "for sentence in readwords1:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq1.keys():\n",
    "            wordfreq1[token] = 1\n",
    "        else:\n",
    "            wordfreq1[token] += 1\n",
    "for sentence in readwords2:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq2.keys():\n",
    "            wordfreq2[token] = 1\n",
    "        else:\n",
    "            wordfreq2[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq1,wordfreq2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "most_freq1 = heapq.nlargest(200, wordfreq1, key=wordfreq1.get)\n",
    "most_freq2 = heapq.nlargest(200, wordfreq2, key=wordfreq2.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors1 = []\n",
    "sentence_vectors2 = []\n",
    "for sentence in readwords1:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    for token in most_freq1:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors1.append(sent_vec)\n",
    "for sentence in readwords2:\n",
    "    sentence_tokens = nltk.word_tokenize(sentence)\n",
    "    sent_vec = []\n",
    "    for token in most_freq2:\n",
    "        if token in sentence_tokens:\n",
    "            sent_vec.append(1)\n",
    "        else:\n",
    "            sent_vec.append(0)\n",
    "    sentence_vectors2.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors1,sentence_vectors2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors3 = np.asarray(sentence_vectors1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sentence_vectors1,columns=wordfreq1.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(sentence_vectors2,columns=wordfreq2.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchstring = \"Web hello intersection data is analysis interdisciplinary subfield usefull\"\n",
    "\n",
    "sc1=0\n",
    "sc2=0\n",
    "for x in searchstring.split(\" \"):\n",
    "        if(x in text1.split(\" \")):\n",
    "            sc1=sc1+1\n",
    "        if(x in text2.split(\" \")):\n",
    "            sc2=sc2+1\n",
    "\n",
    "if(sc1>sc2):\n",
    "    print(\"Doc1 is more relevent\")\n",
    "elif(sc1==sc2):\n",
    "    print(\"Both Documents are equally relevent\")\n",
    "elif(sc1<sc2):\n",
    "    print(\"Doc2 is more relevent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = soup1.findAll(text = re.compile('data'))\n",
    "test2 = soup2.findAll(text = re.compile('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchstring = \"Web hello intersection data is analysis interdisciplinary subfield usefull\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2 is more relevent\n"
     ]
    }
   ],
   "source": [
    "sc1=0\n",
    "sc2=0\n",
    "for x in searchstring.split(\" \"):\n",
    "        if(x in text1.split(\" \")):\n",
    "            sc1=sc1+1\n",
    "        if(x in text2.split(\" \")):\n",
    "            sc2=sc2+1\n",
    "\n",
    "if(sc1>sc2):\n",
    "    print(\"Doc1 is more relevent\")\n",
    "elif(sc1==sc2):\n",
    "    print(\"Both Documents are equally relevent\")\n",
    "elif(sc1<sc2):\n",
    "    print(\"Doc2 is more relevent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 146)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test1),len(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website 2 has highest frequency of 146\n"
     ]
    }
   ],
   "source": [
    "if(len(test1)>len(test2)):\n",
    "    print(\"Website 1 has highest frequency of \" + str(len(test1)))\n",
    "elif(len(test1)==0 and len(test2)==0):\n",
    "    print(\"String not found\")\n",
    "else:\n",
    "    print(\"Website 2 has highest frequency of \" + str(len(test2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
