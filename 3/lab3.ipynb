{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Devang Mehrotra\n",
    "#18BCE0763"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    " \n",
    "import re\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    }
   ],
   "source": [
    "#common words\n",
    "words1 = []    \n",
    "#Group in a list the words common for two text files and show their total count\n",
    "f1 = open(\"AI.txt\").readlines()\n",
    "f2 = open(\"ML.txt\").readlines()\n",
    "if len(f1) != 0 | len(f2) != 0:\n",
    "    uniq1 = set(words for line in f1 for words in line.strip().split())\n",
    "    uniq2 = set(wordss for lines in f2 for wordss in lines.strip().split())\n",
    "    for words in uniq1:\n",
    "        for wordds in uniq2:\n",
    "            if words == wordds:\n",
    "                words1.append(words);\n",
    "               \n",
    "              \n",
    "words1 = [w for w in words1 if not w in stop_words] \n",
    "print(len(words1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['used', 'computer', 'based', 'trying', 'way', 'development', 'While', 'The', 'applications', 'made', 'They', 'language', 'use', 'new', 'machine', 'learning', 'creating', 'developed', 'Applications', 'various', 'think', 'many', 'technology', 'change', 'may', 'science', 'humans', 'developing', 'recognition', 'Intelligence', 'In', 'perform', 'provide', 'work', 'possible', 'associated', 'intelligence', 'like', 'Speech', 'learning,', 'What', 'fields', 'world', 'These', 'people', 'making', 'Artificial', 'programming', 'complex', 'specific', 'AI', 'natural', 'world,', 'learn,', 'paper', 'different', 'data', 'tasks', 'interact', 'increasing', 'intelligent']\n"
     ]
    }
   ],
   "source": [
    "with open('index.txt', 'w') as f:\n",
    "    for item in words1:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "        \n",
    "readwords = []\n",
    "        \n",
    "# opening the text file \n",
    "with open('index.txt','r') as file: \n",
    "   \n",
    "    # reading each line     \n",
    "    for line in file: \n",
    "   \n",
    "        # reading each word         \n",
    "        for word in line.split(): \n",
    "   \n",
    "            # displaying the words            \n",
    "            readwords.append(word)\n",
    "            \n",
    "print(readwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use  -  used\n",
      "comput  -  computer\n",
      "base  -  based\n",
      "tri  -  trying\n",
      "way  -  way\n",
      "develop  -  development\n",
      "while  -  While\n",
      "the  -  The\n",
      "applic  -  application\n",
      "made  -  made\n",
      "they  -  They\n",
      "languag  -  language\n",
      "use  -  use\n",
      "new  -  new\n",
      "machin  -  machine\n",
      "learn  -  learning\n",
      "creat  -  creating\n",
      "develop  -  developed\n",
      "applic  -  Applications\n",
      "variou  -  various\n",
      "think  -  think\n",
      "mani  -  many\n",
      "technolog  -  technology\n",
      "chang  -  change\n",
      "may  -  may\n",
      "scienc  -  science\n",
      "human  -  human\n",
      "develop  -  developing\n",
      "recognit  -  recognition\n",
      "intellig  -  Intelligence\n",
      "In  -  In\n",
      "perform  -  perform\n",
      "provid  -  provide\n",
      "work  -  work\n",
      "possibl  -  possible\n",
      "associ  -  associated\n",
      "intellig  -  intelligence\n",
      "like  -  like\n",
      "speech  -  Speech\n",
      "learning,  -  learning,\n",
      "what  -  What\n",
      "field  -  field\n",
      "world  -  world\n",
      "these  -  These\n",
      "peopl  -  people\n",
      "make  -  making\n",
      "artifici  -  Artificial\n",
      "program  -  programming\n",
      "complex  -  complex\n",
      "specif  -  specific\n",
      "AI  -  AI\n",
      "natur  -  natural\n",
      "world,  -  world,\n",
      "learn,  -  learn,\n",
      "paper  -  paper\n",
      "differ  -  different\n",
      "data  -  data\n",
      "task  -  task\n",
      "interact  -  interact\n",
      "increas  -  increasing\n",
      "intellig  -  intelligent\n",
      "55\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "stems = []\n",
    "lemma = []\n",
    "for w in readwords:\n",
    "    print(ps.stem(w), \" - \", lemmatizer.lemmatize(w))   \n",
    "    stems.append(ps.stem(w))\n",
    "    lemma.append(lemmatizer.lemmatize(w))\n",
    "    \n",
    "frequency1 = {} \n",
    "for word in stems: \n",
    "    count = frequency1.get(word,0) \n",
    "    frequency1[word] = count + 1 \n",
    "frequency_list1 = frequency1.keys()   \n",
    "print(len(frequency_list1))\n",
    "    \n",
    "frequency2 = {} \n",
    "for word in lemma: \n",
    "    count = frequency2.get(word,0) \n",
    "    frequency2[word] = count + 1 \n",
    "frequency_list2 = frequency2.keys()   \n",
    "print(len(frequency_list2))\n",
    "\n",
    "if(len(frequency_list1) <= len(frequency_list2)):\n",
    "    with open('index.txt', 'w') as f:\n",
    "        for item in stems:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "    \n",
    "if(len(frequency_list1) > len(frequency_list2)):\n",
    "    print(\"hello\")\n",
    "    with open('index.txt', 'w') as f:\n",
    "        for item in lemma:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "#os.rename('index.txt', 'final-index.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Devang Mehrotra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('use', 'NN'), ('comput', 'NN'), ('base', 'NN'), ('tri', 'JJ'), ('way', 'NN'), ('develop', 'VB'), ('while', 'IN'), ('the', 'DT'), ('applic', 'JJ'), ('made', 'VBD'), ('they', 'PRP'), ('languag', 'VBP'), ('use', 'VBP'), ('new', 'JJ'), ('machin', 'NN'), ('learn', 'VBP'), ('creat', 'NN'), ('develop', 'VB'), ('applic', 'JJ'), ('variou', 'NN'), ('think', 'VBP'), ('mani', 'NNS'), ('technolog', 'VBP'), ('chang', 'NN'), ('may', 'MD'), ('scienc', 'VB'), ('human', 'JJ'), ('develop', 'VB'), ('recognit', 'NN'), ('intellig', 'NN'), ('In', 'IN'), ('perform', 'NN'), ('provid', 'NN'), ('work', 'NN'), ('possibl', 'NN'), ('associ', 'JJ'), ('intellig', 'NN'), ('like', 'IN'), ('speech', 'NN'), ('learning,', 'VBP'), ('what', 'WP'), ('field', 'NN'), ('world', 'NN'), ('these', 'DT'), ('peopl', 'NNS'), ('make', 'VBP'), ('artifici', 'JJ'), ('program', 'NN'), ('complex', 'JJ'), ('specif', 'NN'), ('AI', 'NNP'), ('natur', 'CC'), ('world,', 'JJ'), ('learn,', 'JJ'), ('paper', 'NN'), ('differ', 'NN'), ('data', 'NNS'), ('task', 'NN'), ('interact', 'NN'), ('increas', 'JJ'), ('intellig', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "finalwords = []\n",
    "        \n",
    "# opening the text file \n",
    "with open('index.txt','r') as file: \n",
    "   \n",
    "    # reading each line     \n",
    "    for line in file: \n",
    "   \n",
    "        # reading each word         \n",
    "        for word in line.split(): \n",
    "   \n",
    "            # displaying the words            \n",
    "            finalwords.append(word)\n",
    "tagged = nltk.pos_tag(finalwords)     \n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0    1\n",
      "0        use   NN\n",
      "1     comput   NN\n",
      "2       base   NN\n",
      "3        tri   JJ\n",
      "4        way   NN\n",
      "..       ...  ...\n",
      "56      data  NNS\n",
      "57      task   NN\n",
      "58  interact   NN\n",
      "59   increas   JJ\n",
      "60  intellig   NN\n",
      "\n",
      "[61 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(tagged)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
